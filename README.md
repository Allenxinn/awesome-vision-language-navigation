# Awesome Vision Language Navigation
 A curated list for VLN.

## VLM-based
| Title| Year| Published | CODE |
| :---------| :--------------: | :------------: | :------------: |
|[SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation](https://arxiv.org/abs/2511.21135)| 2025.11 | Arxiv | [Prject](https://amap-eai.github.io/SocialNav/)| 
|[Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation](https://arxiv.org/abs/2511.08935)| 2025.11 | Arxiv | [Code](https://github.com/mrwangyou/SCOPE)![Github stars](https://img.shields.io/github/stars/mrwangyou/SCOPE)| 
|[PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory](https://arxiv.org/abs/2511.06840v1)| 2025.11 | Arxiv | - | 
|[MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/abs/2511.17889)| 2025.11 | Arxiv | [Code](https://github.com/AIGeeksGroup/MobileVLA-R1)![Github stars](https://img.shields.io/github/stars/AIGeeksGroup/MobileVLA-R1) |
|[InternVLA-N1: An Open Dual-System Vision-Language Navigation Foundation Model with Learned Latent Plans](https://internrobotics.github.io/internvla-n1.github.io/static/pdfs/InternVLA_N1.pdf)| 2025.10 | - | [Code](https://github.com/InternRobotics/InternNav)![Github stars](https://img.shields.io/github/stars/InternRobotics/InternNav) | 
|[Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation](https://arxiv.org/abs/2510.08553)| 2025.10 | Arxiv | [Code(coming soon)](https://github.com/xyz9911/Memoir) | 
|[VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation](https://arxiv.org/abs/2510.20818)| 2025.10 | Arxiv | [Code](https://github.com/vamos-vla/vamos)[Github stars](https://img.shields.io/github/stars/vamos-vla/vamos)| 
|[CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)| 2025.10 | AAAI2026 | - | 
|[NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://www.arxiv.org/abs/2510.16457)| 2025.10 | - | [Code](https://github.com/woyut/NavQ_ICCV25)![Github stars](https://img.shields.io/github/stars/woyut/NavQ_ICCV25) | 
|[VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)| 2025.9 | Arxiv | [Code](https://github.com/VLN-Zero/vln-zero.github.io)![Github stars](https://img.shields.io/github/stars/VLN-Zero/vln-zero.github.io)| 
|[StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling](https://arxiv.org/abs/2507.05240) | 2025.7 | Arxiv | [Code](https://github.com/InternRobotics/StreamVLN) ![Github stars](https://img.shields.io/github/stars/InternRobotics/StreamVLN) | 
|[VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.17221)| 2025.1 | Arxiv | [Code](https://github.com/Qi-Zhangyang/GPT4Scene-and-VLN-R1) ![Github stars](https://img.shields.io/github/stars/Qi-Zhangyang/GPT4Scene-and-VLN-R1) | 
|[NaVILA: Legged Robot Vision-Language-Action Model for Navigation](https://arxiv.org/abs/2412.04453)| 2024.12 | RSS2025 | [Prject](https://navila-bot.github.io/) ![Github stars](https://img.shields.io/github/stars/AnjieCheng/NaVILA) | 


## Map-based
| Title| Year| Published | CODE |
| :---------| :--------------: | :------------: | :------------: |
|[VLN-KHVR: Knowledge-and-History Aware Visual Representation for Continuous Vision-and-Language Navigation](https://ieeexplore.ieee.org/abstract/document/11127961)| 2025.5 | ICRA 2025 | - | 
|[ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2304.03047)| 2024.8 | TPAMI 2024 | [Code](https://github.com/MarSaKi/ETPNav) ![Github stars](https://img.shields.io/github/stars/MarSaKi/ETPNav) | 